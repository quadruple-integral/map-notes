\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{physics}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{centernot}

% commands
\newcommand{\deq}{\vcentcolon=}
\newcommand{\idd}{\text{Ä‘}}
\newcommand{\nimplies}{\centernot\implies}
\newcommand{\vc}[1]{\boldsymbol{#1}}

% margin settings
\geometry{
    a4paper,
    left=7mm,
    right=7mm,
    top=2cm,
    bottom=7mm
}

% testing
\usepackage{blindtext}

% proof environments
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark} % unnumbered remarks

% header and footer
\pagestyle{fancy}
\fancyfoot{} % removes footer
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\fancyhead[L]{Honours algebra}
\fancyhead[R]{\thepage}

\begin{document}

\begin{multicols*}{3}
\noindent

\subsubsection*{D: Functions}
A function $f:X\rightarrow Y$ is an assignment
of an element of $Y$ to \underline{each} element of $X$.
\begin{enumerate}
    \item $f$ is \textbf{injective} if:
    \begin{align*}
        &\forall x_1,x_2\in X;
        f(x_1)=f(x_2) \\ &\implies x_1=x_2
    \end{align*}
    and this implies that $|X|\leq|Y|$.

    \item $f$ is \textbf{surjective} if:
    $$\forall y\in Y;\exists x\in X: y=f(x)$$
    and this implies that $|X|\geq|Y|$.

    \item $f$ is \textbf{bijective}
    if it is injective and surjective.
\end{enumerate}
\begin{center}
    \includegraphics[scale=0.2]{f00.png}
\end{center}

\subsubsection*{D: Groups}
A group $G$ is a set with a composition
operator $(\circ)$ such that $\forall x,y,z,\in G$:
\begin{enumerate}
    \item $x\circ y=xy\in G$
    
    \item $(xy)z=x(yz)$
    
    \item $\exists e\in G: ex=xe=x$
    
    \item $\exists x^{-1}\in G: xx^{-1}=x^{-1}x=e$.
\end{enumerate}
$G$ is \textbf{Abelian} if 
$\forall x,y\in G; xy=yx$.

\subsubsection*{D1.2.1(i): Fields}
A field $F$ is a set defined with addition
and multiplication such that:
\begin{enumerate}
    \item $(+):F\times F\rightarrow F;
    (\lambda,\mu)\mapsto\lambda+\mu$

    \item $(\cdot):F\times F\rightarrow F;
    (\lambda,\mu)\mapsto\lambda\cdot\mu$

    \item $\exists (-\lambda)\in F:
    \lambda+(-\lambda)=0_F$

    \item $\exists (\lambda^{-1})\in F:
    \lambda\cdot(\lambda^{-1})=1_F$ \\
    except when $\lambda=0$.

    \item $(+)$ and $(\cdot)$
    are associative, \\ commutative
    and distributive.
\end{enumerate}

\subsubsection*{Remark}
$(F,+)$ and $(F\hspace{0.02in}\backslash
\hspace{0.02in}\{0_F\},\cdot)$ are groups.

\subsubsection*{Remark}
Let $n$ be prime or a prime power.
Then $\mathbb{F}_n$ is a finite field with $n$ elements
under modulo $n$.
Also, $\mathbb{Q}$ and $\mathbb{R}$ are fields.

\subsubsection*{D1.2.1(ii): Vector spaces}
A vector space $V$ over a field $F$ is an 
\textcolor{red}{Abelian group} $V\deq(V,+)$ with mapping:
$$F\times V\rightarrow V;
(\lambda,\vc{v})\mapsto
\lambda\vc{v}$$
where for $\forall\lambda, \mu\in F$ and
$\forall\vc{v},\vc{w}\in V$:
\begin{enumerate}
    \item
    $\lambda(\vc{v}+\vc{w})
    =(\lambda\vc{v})+(\lambda\vc{w})$

    \item $(\lambda+\mu)\vc{v}
    =(\lambda\vc{v})+(\mu\vc{v})$

    \item $\lambda(\mu\vc{v})=(\lambda\mu)\vc{v}$
    
    \item $1_F\vc{v}=\vc{v}$
\end{enumerate}
and is known as a \textbf{$F$-vector space}.

\subsubsection*{Remark}
Let $V$ be a $F$-vector space and $\vc{v}\in V$.
\begin{enumerate}
    \item $0\vc{v}=0$
    \item $(-1)\vc{v}=-\vc{v}$
    \item $\lambda\vc{0}=\vc{0}$
    for $\forall\lambda\in F$.
\end{enumerate}

\subsubsection*{D: Cartesian products}
The \textbf{cross product} of sets $X_1,\dots,X_n$ is:
$$X_1\times\cdots\times X_n
\deq\{(x_1,\dots,x_n):x_i\in X_i\}$$
with bijection $X^n\times X^m\rightarrow X^{n+m}$.

The \textbf{projection} of a cross product is:
\begin{align*}
    \text{pr}_i &:X_1\times\cdots\times X_n
    \rightarrow X_i; \\
    &(x_1,\dots,x_n)\mapsto x_i.
\end{align*}

\subsubsection*{D1.4.1: Vector subspaces}
A vector subspace $U$ of $F$-vector space $V$
has the following properties:
\begin{enumerate}
    \item $U\subset V$ and $\vc{0}\in U$.
    \item Let $\vc{u},\vc{v}\in U$
    and $\lambda\in F$. \\ Then
    $\vc{u}+\vc{v}\in U$ and $\lambda\vc{u}\in U$.
\end{enumerate}
and is also a vector space.

\subsubsection*{P1.4.5}
Let $T\subset V$ where $V$ is a $F$-vector space.
Then for all vector subspaces containing $T$,
there exists a \underline{smallest} vector subspace:
$$\text{span}(T)=\langle T\rangle_F\subset V$$
known as the vector subspace generated by $T$,
or the span of $T$.

\subsubsection*{D1.4.7: Generating set}
Let $T\subset V$ where $V$ is a $F$-vector space.
Set $T$ is a \textbf{generating set} of $V$ if:
$$\text{span}(T)=V$$
and is the linear combination of vectors in $T$
over field $F$. $V$ is \textbf{finitely generated}
if its generating set $T$ is finite.

\subsubsection*{D1.4.9: Power sets}
The power set of set $X$ is:
$$\mathcal{P}(X)\deq\{U:U\subseteq X\}.$$
Let $\mathcal{U}\subseteq\mathcal{P}(X)$. Then:
$$\bigcup_{U\in\mathcal{U}}U
\deq\{x\in X:(\exists U\in\mathcal{U}:x\in U)\}$$
$$\bigcap_{U\in\mathcal{U}}U
\deq\{x\in X:\forall U\in\mathcal{U};x\in U\}.$$

\subsubsection*{D1.5.1: Linear independence}
Let $V$ be a $F$-vector space and $L\subseteq V$. \\
Subset $L$ is \textbf{linearly independent} if:
\begin{align*}
    &\alpha_1\vc{v}_1+\dots+\alpha_r\vc{v}_r=\vc{0} \\
    &\implies \alpha_1=\dots=\alpha_r=0
\end{align*}
for $\vc{v}_i\in L$
and is pairwise distinct.

\subsubsection*{Remark}
$L$ is linearly dependent if some $\alpha_i\neq0$.

\subsubsection*{D1.5.8: Basis}
A basis of a vector space $V$ is a \textbf{linearly independent}
\underline{generating set} in $V$.

\subsubsection*{T1.5.11: Basis evaluation mappings}
Let $V$ be a $F$-vector space.

Then $A=\{\vc{v}_1,\dots,\vc{v}_r\}$ is a basis of $V$
\textbf{if{}f} the following \textcolor{red}{evaluation mapping}:
$$\Phi_A:F^r\rightarrow V;$$
$$(\alpha_1,\dots,\alpha_r)\mapsto
\alpha_1\vc{v}_1+\dots+\alpha_r\vc{v}_r$$
is a bijection.

\subsubsection*{Remark}
$\Phi$ is surjective if $A$ is generating.

\subsubsection*{T1.5.12}
Let $V$ be a vector space and $E\subseteq V$.
Then the following statements are equivalent:
\begin{enumerate}
    \item $E$ is a basis of $V$.
    
    \item $E$ is minimal among all generating sets,
    or that $E\hspace{0.02in}\backslash\hspace{0.02in}
    \{\vc{e}\}$ is not a basis for $\forall\vc{e}\in E$.

    \item $E$ is maximal amongst all linearly independent subsets.
    i.e. $E\hspace{0.01in}\cup\hspace{0.01in}\{\vc{v}\}$
    is linearly dependent for $\forall\vc{v}\in V$.
\end{enumerate}

\subsubsection*{C1.5.13}
Every finitely generated vector space has a finite basis.

\subsubsection*{T1.5.14}
Let $V$ be a vector space.
\begin{enumerate}
    \item Let $L\subseteq V$ be linearly independent and
    set $E$ be minimal amongst all generating sets of $V$.
    Let $L\subseteq E$.
    Then $E$ is a basis of $V$.

    \item Let $E\subseteq V$ be a generating set and
    $L$ be maximal amongst all linearly independent
    subsets of $V$. 
    
    Let $L\subseteq E$. Then $E$ is a basis of $V$.
\end{enumerate}

\subsubsection*{D1.5.15}
Let $X$ be a set and $F$ be a field. Then:
$$\text{maps}(X,F)\deq\{f:(\forall f:X\rightarrow F)\}$$
and is a $F$-vector space under pointwise addition and
multiplication via scalars.

Let $F\langle X\rangle\subseteq\text{maps}(X,F)$ be
the subset of all mappings that sends all but finitely many
elements of $X$ to $0$:
$$F\langle X\rangle\deq
\left\{f:\bigl(\forall f:X\rightarrow\{0\}\bigr)\right\}.$$
It contains all linear combinations of $X$
in $F$ and forms a vector subspace.

\subsubsection*{T1.5.16}
Let $V$ be a $F$-vector space.

Then $(\vc{v}_i)_{i\in I}$ is a basis for $V$ \textbf{if{}f}:
\begin{align*}
    \forall\vc{v}\in V;\exists!(a_i)_{i\in I}\subseteq F:
    \vc{v}=\sum_{i\in I}a_i\vc{v}_i.
\end{align*}

\subsubsection*{T1.6.1}
Let $V$ be a vector space.
Let $L\subset V$ be a linearly independent subset
and $E\subseteq V$ a generating set.
Then $|L|\leq|E|$.

\subsubsection*{T1.6.2: Steinitz exchange theorem}
Let $V$ be a vector space, $L\subset V$ be a finite
linearly independent subset and $E\subseteq V$
be a generating set.

Then there exists an \textbf{injective} function
$\phi:L\rightarrow E$ such that:
$$\bigl(E\hspace{0.02in}\backslash\hspace{0.02in}
\phi(L)\bigr)\cup L
\hspace{0.05in}\text{is a generating set for $V$.}$$

\subsubsection*{L1.6.3: Exchange lemma}
Let $V$ be a vector space. Let $M\subset V$ be a finite
linearly independent subset and $E\subseteq V$
be a generating set where $M\subseteq E$.

If $\exists\vc{w}\in V\hspace{0.02in}\backslash\hspace{0.02in} M$
such that set $M\cup\{\vc{w}\}$ is linearly independent then:
$$\exists\vc{e}\in E\hspace{0.02in}\backslash\hspace{0.02in} M:
\bigl(E\hspace{0.02in}\backslash\hspace{0.02in}
\vc{e}\bigr)\cup\{\vc{w}\}\hspace{0.04in}\text{is generating.}$$

\subsubsection*{C1.6.4}
Let $V$ be a finitely generated vector space.
\begin{enumerate}
    \item $V$ has finite basis.
    
    \item $V$ cannot have infinite basis.
    
    \item Any two basis of $V$ have the
    same number of elements.
\end{enumerate}

\subsubsection*{D1.6.5: Dimension}
The dimension of finite $F$-vector space $V$ is the
cardinality of one of its basis. 

For infinite vector spaces: $\dim(V)=\infty$.
We also define $\dim(\{\vc{0}\})\deq0$.

\subsubsection*{C1.6.7}
Let $V$ be a finitely generated vector space.
\begin{enumerate}
    \item Every linearly independent $L\subseteq V$
    has \textbf{at most} $\dim(V)$ elements and \\
    if $|L|=\dim(V)$ then $L$ is a basis.

    \item Every generating set $E\subseteq V$ has \\
    \textbf{at least} $\dim(V)$ elements and \\
    if $|E|=\dim(V)$ then $E$ is a basis.
\end{enumerate}

\subsubsection*{C1.6.8}
A proper vector subspace of a vector space 
with finite dimension has itself a strictly smaller dimension.

\subsubsection*{T1.6.10: Dimension theorem}
Let $V$ be a vector space and 
$U,W\subseteq V$ be vector subspaces. Then:
\begin{align*}
    &\dim(U+W)+\dim(U\cap W) \\
    &=\dim(U)+\dim(W)
\end{align*}
where $U+W\deq\langle U\hspace{0.02in}
\cup\hspace{0.02in}W\rangle\subseteq V$.

\subsubsection*{D1.7.1: Linear mappings}
Let $V$ and $W$ be $F$-vector spaces. \\
A mapping $f:V\rightarrow W$ is \textbf{$F$-linear} or a
\textbf{homomorphism} of vector spaces if for
$\forall\vc{v}_1,\vc{v}_2\in V$ and $\forall\lambda\in F$:
\begin{enumerate}
    \item $f(\vc{v}_1+\vc{v}_2)=f(\vc{v}_1)+f(\vc{v}_2)$
    \item $f(\lambda\vc{v}_1)=\lambda f(\vc{v}_1)$.
\end{enumerate}
Furthermore bijective linear mappings are an \textbf{isomorphism}
of vector spaces.

A homomorphism from a vector space to itself
is an \textbf{endomorphism}.

An isomorphism of a vector space to itself
is an \textbf{automorphism}.

\subsubsection*{D1.7.5: Fixed points}
In a linear mapping a fixed point is sent to itself.
Given mapping $f:X\rightarrow X$
the \textbf{set of fixed points} is:
$$X^f=\{x\in X:f(x)=x\}.$$

\subsubsection*{D1.7.6: Complementary subspaces}
Vector subspaces $V_1,V_2$ of vector space $V$
are \textbf{complementary} if the \textcolor{red}{direct sum}
of vector subspaces is bijective:
$$\oplus:V_1\times V_2\rightarrow V;
(\vc{v}_1,\vc{v}_2)\mapsto\vc{v}_1+\vc{v}_2.$$
i.e. $V_1\oplus V_2=V$.

\subsubsection*{T1.7.7}
Let $n\in\mathbb{N}$ and $V$ a $F$-vector space. \\
$V$ is isomorphic to $F^n$
\textbf{if{}f} $\dim(V)=n$.

\subsubsection*{L1.7.8}
Let $V,W$ be $F$-vector spaces and let $B$
be a basis of $V$. Then the following mapping:
\begin{align*}
    \hom_F(V,W)\rightarrow\text{maps}(B,W);
    f\mapsto f_B
\end{align*}
is a bijection.
The set of all linear maps or
homomorphisms from $V$ to $W$ is:
$$\hom_F(V,W)\subseteq\text{maps}(B,W).$$

\subsubsection*{P1.7.9}
Let $f:V\rightarrow W$ be a linear mapping,
where $V,W$ are vector spaces.
\begin{enumerate}
    \item If $f$ is injective, there exists map
    $g:W\rightarrow V$ such that $g\circ f=\text{id}_V$.
    i.e. it has a \textbf{left inverse}.
    
    \item If $f$ is surjective, there exists map
    $g:W\rightarrow V$ such that $f\circ g=\text{id}_W$.
    i.e. it has a \textbf{right inverse}.
\end{enumerate}

\subsubsection*{D1.8.1: Image and kernel}
Let $f:V\rightarrow W$ be a linear mapping. \\
The \textbf{image} of this linear mapping $f$ is:
\begin{align*}
    \text{im}(f)&\deq f(V) \\
    &=\{\vc{w}\in W:
    \forall\vc{v}\in V;\vc{w}=f(\vc{v})\}
\end{align*}
and is a vector subspace of $W$.

The \textbf{kernel} of this linear mapping $f$ is:
$$\ker(f)\deq f^{-1}(\vc{0})
=\{\vc{v}\in V:f(\vc{v})=\vc{0}\}$$
and is a vector subspace of $V$.

$\vc{0}=e_H$ for group homomorphisms.

\subsubsection*{L1.8.2}
A linear mapping $f:V\rightarrow W$ is injective
\textbf{if{}f} $\ker(f)=\{\vc{0}\}$.

\subsubsection*{T1.8.4: Rank-nullity theorem}
Let $f:V\rightarrow W$ be a linear mapping and
$V,W$ are vector spaces. Then:
$$\dim(V)=\dim\Bigl(\ker(f)\Bigr)
+\dim\Bigl(\text{im}(f)\Bigr).$$

\subsubsection*{T2.1.1: Matrix mappings}
Let $F$ be a field and $m,n\in\mathbb{N}$.

Then there exists a \underline{bijection}:
$$M:\hom_F(F^m,F^n)\rightarrow\text{mat}(n\times m;F);$$
$$f\mapsto[f]$$
and attaches each linear mapping $f$ with its
\textbf{representing matrix} $M(f)\deq[f]$.

\subsubsection*{Remark}
The set of $n\times m$ matrices in $F$ is defined:
$$\text{mat}(n\times m;F).$$
i.e. 
matrices with \textcolor{red}{$n$ rows}
and \textcolor{red}{$m$ columns}.


\subsubsection*{D2.1.6: Matrix products}
The product $A\circ B=AB$ for $A$ is $n\times m$,
$B$ is $m\times\ell$ and $AB$ is $n\times\ell$ is defined as:
$$(AB)_{ik}=\sum_{j=1}^{m}A_{ij}B_{jk}$$
with the following mapping:
\begin{align*}
    &\text{mat}(n\times m;F)\times\text{mat}(m\times\ell;F) \\
    &\rightarrow\text{mat}(n\times\ell;F);(A,B)\mapsto AB.
\end{align*}

\subsubsection*{T2.1.8}
Let $g:F^{\ell}\rightarrow F^m$ and
$f:F^{m}\rightarrow F^{n}$ be linear mappings.
Then $[f\circ g]=[f]\circ[g]$.

\subsubsection*{P2.1.9}
Let $A,A'$ be $n\times m$,
$B,B'$ be $m\times\ell$ and
$C,C'$ be $\ell\times k$.
Denote $I=I_m$
as the $m\times m$ identity matrix. Then:
\begin{enumerate}
    \item $(A+A')B=AB+A'B$
    \item $A(B+B')=AB+AB'$
    \item $IB=B$
    \item $AI=A$
    \item $(AB)C=A(BC)$.
\end{enumerate}

\subsubsection*{D2.2.1: Invertible matrices}
A matrix $A$ is \textbf{invertible} if:
$$\exists B,C:
BA=I\hspace{0.05in}\text{and}\hspace{0.05in}AC=I.$$

\subsubsection*{D2.2.2: Elementary matrices}
Elementary matrices are square matrices that differs from
the identity matrix by at most one entry.

\subsubsection*{T2.2.3}
Every square matrix with entries in a field can be written
as a \underline{product} of elementary matrices.

\subsubsection*{D2.2.4: Smith normal form}
Matrices with \textbf{only} non-zero entries along the diagonal are
in Smith normal form:
$$\begin{pmatrix}
    1 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0
\end{pmatrix}.$$

\subsubsection*{T2.2.5}
Let $A$ be an $n\times m$ matrix. Then:
$$PAQ\hspace{0.05in}\text{is of Smith normal form}$$
where $P$ and $Q$ are invertible.

\subsubsection*{Remark}
$\rank(A)=\rank(PAQ)$.

\subsubsection*{D2.2.7: Column and row rank}
Let matrix $A\in\text{mat}(n\times m;F)$.

The column rank of $A$ is the dimension of the subspace
of $F^n$ generated by the columns of $A$.

Similarly the row rank of $A$ is the dimension of the subspace
of $F^m$ generated by the rows of $A$.

\subsubsection*{T2.2.8}
Column and row ranks are equal.

\subsubsection*{D2.2.9: Full rank matrices}
Let $A$ be $n\times m$ with entries in $F$. \\
$A$ is \textbf{full rank} if $\rank(A)=\min(m,n)$.

Let $A=[a]$ with mapping $a:F^m\rightarrow F^n$.
Then $\dim\bigl(\text{im}(a)\bigr)\deq\rank(A)$.

\subsubsection*{T2.3.1: Representing matrices}
Let $V$ and $W$ be $F$-vector spaces
with bases $A=\{\vc{v}_1,\dots,\vc{v}_m\}$
s.t. $\langle A\rangle=V$
and $B=\{\vc{w}_1,\dots,\vc{w}_n\}$
s.t. $\langle B\rangle=W$.

Then for every linear map $f:V\rightarrow W$
there exists a \textbf{representing matrix}:
$$({}_{B}[f]_A)_{ij}=a_{ij}$$
$$f(\vc{v}_j)=a_{1j}\vc{w}_1+\dots+a_{nj}\vc{w}_n\in W$$
which produces the following bijection:
$$M_B^A:\hom_F(V,W)\rightarrow\text{mat}(n\times m;F);$$
$$f\mapsto{}_B[f]_A$$
and $M_B^A(f)={}_B[f]_A$ is the representing matrix
of linear mapping $f$ 
with respect to bases $A$ and $B$.

If $A$ and $B$ are standard bases then $[f]$.

\subsubsection*{T2.3.2}
Let $U,V,W$ be $F$-vector spaces with finite dimension
and bases $A,B,C$ respectively.

If $f:U\rightarrow V$ and $g:V\rightarrow W$
are linear mappings then
${}_C[g\circ f]_A={}_C[g]_B\circ{}_B[g]_A$.

\subsubsection*{D2.3.3: Vector representations}
Let $V$ be a finite dimensional vector space
with basis $A=\{\vc{v}_1,\dots,\vc{v}_m\}$.
Then:
$$\Phi_A^{-1}:V\rightarrow F^r;
\vc{v}\mapsto{}_A[\vc{v}]$$
is a bijection and the column vector ${}_A[\vc{v}]$
is known as the \textcolor{red}{representation
of vector $\vc{v}$ with respect to basis $A$}.

\subsubsection*{T2.3.4}
Let $V,W$ be finite dimensional $F$-vector spaces
with bases $A$ and $B$ respectively.

Let $f:V\rightarrow W$ be a linear mapping.
Then ${}_B[f(\vc{v})]={}_B[f]_A\circ {}_A[\vc{v}]$
for $\forall\vc{v}\in V$.

\subsubsection*{D2.4.1}
Let $V$ be a $F$-vector space
and let sets
$A=\{\vc{v}_1,\dots,\vc{v}_n\}$
and $B=\{\vc{w}_1,\dots,\vc{w}_n\}$
be bases of $V$.
The representation matrix of the identity mapping:
$$\text{id}_V:V\rightarrow V;
\vc{v}\mapsto\vc{v}$$
is a \textbf{change of basis matrix}
${}_B[\text{id}_V]_A$ with entries $a_{ij}$
given by definition:
$$\vc{v}_j=\sum_{i=1}^{n}a_{ij}\vc{w}_i.$$

\subsubsection*{T2.4.3: Change of basis}
Let $V$ and $W$ be finite dimensional vector spaces
with linear mapping $f:V\rightarrow W$.
Let $A,A'$ be ordered bases of $V$ and
$B,B'$ be ordered bases of $W$. Then:
$${}_{B'}[f]_{A'}={}_{B'}[\text{id}_W]_{B}
\circ{}_{B}[f]_{A}\circ{}_{A}[\text{id}_V]_{A'}.$$

\subsubsection*{C2.4.4}
Let $V$ be a finite dimensional vector space and let
$f:V\rightarrow V$ be an endomorphism.
Let $A,A'$ be bases of $V$. Then:
$${}_{A'}[f]_{A'}={}_{A}[\text{id}_V]^{-1}_{A'}
\circ{}_{A}[f]_{A}\circ{}_{A}[\text{id}_V]_{A'}.$$

\subsubsection*{T2.4.5}
Let $V$ and $W$ be finite dimensional vector spaces and let
$f:V\rightarrow W$ be linear.

Then there exists a basis $A$ of $V$ and 
a basis $B$ of $W$ such that the representing matrix
${}_B[f]_A$ has nonzero entries only on the diagonal.

\subsubsection*{D2.4.6: Trace}
The trace of a $n\times n$ matrix $A$ is the sum of its diagonal
entries:
$$\tr(A)=\sum_{i=1}^{n}a_{ii}.$$

\subsubsection*{D3.1.1: Rings}
A \textbf{ring} $R$ is a set equipped with \underline{addition}
and \underline{multiplication} that satisfy:
\begin{enumerate}
    \item $(R,+)$ is an \textbf{Abelian group} with additive
    identity $0_R\in R$.
    
    \item $(R,\cdot)$ is a \textbf{monoid}, meaning that:
    $$(\cdot):R\times R\rightarrow R;(a,b)\mapsto a\cdot b$$
    is associative with identity element $1=1_R\in R$ such that:
    $$\forall a,b,c\in R;(a\cdot b)\cdot c
    =a\cdot(b\cdot c),$$
    $$a\cdot1=1\cdot a=a$$
    yet $a\cdot b\neq b\cdot a$ in general.

    \item Multiplication in $R$ with respect to addition
    is distributive:
    $$a\cdot(b+c)=(a\cdot b)+(a\cdot c)$$
    $$(b+c)\cdot a=(b\cdot a)+(c\cdot a)$$
    for $\forall a,b,c\in R$.
\end{enumerate}
For \textbf{nonzero} rings $0_R\neq1_R$.

\textbf{Division rings} are rings where nonzero elements
have multiplicative inverses.

\subsubsection*{P3.1.7}
A natural number is divisible by $3$ if
the sum of its digits is divisble by $3$.

\subsubsection*{D3.1.8: Fields}
A \textbf{field} $F$ is a nonzero \textcolor{red}{commutative} ring \\
with \textbf{multiplicative} inverses to every
\underline{nonzero} element:
$$\forall a\in F;\exists a^{-1}\in F:
aa^{-1}=a^{-1}a=1.$$
i.e. a commutative division ring.

\subsubsection*{P3.1.11}
$\mathbb{Z}/m\mathbb{Z}$
is a field \textbf{if{}f} $m$ is prime.

\subsubsection*{L3.2.1}
Let $R$ be a ring and $a,b\in R$. Then:
\begin{enumerate}
    \item $0a=a0=0$
    \item $(-a)b=-(ab)=a(-b)$
    \item $(-a)(-b)=ab$.
\end{enumerate}

\subsubsection*{D3.2.3}
Let $m\in\mathbb{Z}$. Then $m$th multiple $ma$ of
$a\in(R,+)$ is $ma=\underbrace{a+\dots+a}_
\text{$m$ times}$ if $m>0$.

$0a\deq0$ and if $m<0$, $(-m)a=-(ma)$.

\subsubsection*{L3.2.4}
Let $R$ be a ring where $a,b\in R$ and
$m,n\in\mathbb{Z}$. Then:
\begin{enumerate}
    \item $m(a+b)=ma+mb$
    \item $(m+n)a=ma+na$
    \item $m(na)=(mn)a$
    \item $m(ab)=(ma)b=a(mb)$
    \item $(ma)(nb)=(mn)(ab).$
\end{enumerate}

\subsubsection*{D3.2.6: Units}
Let $R$ be a ring. An element $r\in R$
is a \textbf{unit} if it has a
\textcolor{red}{multiplicative} \underline{inverse}:
$$\exists r^{-1}\in R: rr^{-1}=r^{-1}r=1_R.$$

\subsubsection*{P3.2.9: Group of units}
$R^{\times}$ is the \textbf{set of units} in ring $R$
and forms a group under multiplication.

\subsubsection*{D3.2.11: Divisor of zero}
Let $\textcolor{red}{r\neq0_R}\in R$ where $R$ is a ring.
Then element $r$ is a \textbf{divisor of zero} if:
$$\exists s\in R: rs=0_R\hspace{0.05in}
\text{or}\hspace{0.05in}sr=0_R.$$

\subsubsection*{D3.2.12: Integral domains}
Intgeral domains are commutative rings
with \textbf{no} divisors of zeros.

\subsubsection*{P3.2.15: Cancellation law}
Let $a,b,c\in R$ for $R$ is an integral domain.
If $ab=ac$ and $a\neq0$ then $b=c$.

\subsubsection*{P3.2.16}
Let $m\in\mathbb{N}$. Then $\mathbb{Z}/m\mathbb{Z}$ 
is an integral domain \textbf{if{}f} $m$ is prime.

\subsubsection*{T3.2.17}
Every finite integral domain is a field.

\subsubsection*{Remark}
If $|R|<\infty$ then $f:R\rightarrow R$ is surjective.

\subsubsection*{D3.3.2: Polynomial rings}
$R[X]$ is a ring of polynomials over $R$ 
with zero and identity: $0,1\in R$. If $P\in R[X]$:
$$P=a_0+a_1 X+a_2 X^2+\dots+a_m X^m$$
with $\deg(P)=m\geq0$ and $a_i\in R$.

\subsubsection*{L3.3.3}
Let $R$ be a ring and let $P,Q\neq0\in R[X]$.
\begin{enumerate}
    \item $\deg(PQ)=\deg(P)+\deg(Q)$
    \item If $R$ is an integral domain then
    so is polynomial ring $R[X]$.
\end{enumerate}

\subsubsection*{T3.3.4}
Let $R$ be an integral domain and
let $P,Q\in R[X]$ where $\deg(Q)\leq\deg(P)$
and that polynomial $Q$ is a \textbf{monic}.

Then $\exists!A,B\in R[X]:P=AQ+B$ and either
$\deg(B)<\deg(Q)$ or $B=0$.

\subsubsection*{Remark}
A polynomial $Q$ is monic if:
$$Q=q_0+\dots+q_m X^m$$
where $q_m=1$.

\subsubsection*{D3.3.6}
Let $R$ be a commutative ring and let $P\in R[X]$
be a polynomial. Then:
$$R[X]\rightarrow\text{maps}(R,R)$$
where we \textbf{evaluate} $P(\lambda)$ for $\lambda\in R$:
$$P(X)\mapsto\{P_{\lambda}:R\rightarrow R;
\lambda\mapsto P(\lambda)\}.$$
If $P(\lambda)=0$ then $\lambda$ is a 
\textbf{root} of $P$.

\subsubsection*{P3.3.9}
Let $R$ be a commutative ring, let $\lambda\in R$
and $P(X)\in R[X]$. Then $P(\lambda)=0$ \textbf{if{}f}:
$$P(X)=(X-\lambda)Q(X)$$
where $Q(X)\in R[X]$.

\subsubsection*{T3.3.10}
Polynomial
$P\neq0\in R[X]\hspace{0.02in}\backslash\hspace{0.02in}\{0\}$
has at most $\deg(P)$ roots in integral domain $R$.

\subsubsection*{D3.3.11: Algebraically closed field}
A field $F$ is algebraically closed if every
$P\in F[X]\hspace{0.02in}\backslash\hspace{0.02in}F$
has a root in field $F$.

\subsubsection*{T3.3.13: FTA}
Field $\mathbb{C}$ is algebraically closed.

\subsubsection*{T3.3.14}
Let field $F$ be algebraically closed. Then every
$P\in F[X]\hspace{0.02in}\backslash\hspace{0.02in}\{0\}$
decomposes into:
$$P=c(X-\lambda_1)\dots(X-\lambda_n)$$
where $c\in F^{\times}$ and $\lambda_1,\dots,\lambda_n\in F$.

\subsubsection*{D3.4.1: Ring homomorphisms}
Let $R$ and $S$ be rings. $f:R\rightarrow S$ is a ring
homomorphism if for all $x,y\in R$:
$$f(x+y)=f(x)+f(y)$$
$$f(xy)=f(x)f(y).$$

\subsubsection*{Remark}
If $f:\mathbb{Z}/m\mathbb{Z}\rightarrow
\mathbb{Z}/n\mathbb{Z}$ is a homomorphism
and $f(1)=x\in\mathbb{Z}/m\mathbb{Z}$ then:
\begin{enumerate}
    \item $x^2=x \pmod n$
    \item $mx=0 \pmod n$, with $\gcd(m,n)$ \\
    \underline{evenly} spaced solutions.
\end{enumerate}

\subsubsection*{L3.4.5}
Let $R$ and $S$ be rings. Let $f:R\rightarrow S$ be a
\textbf{ring homomorphism}.
Then for all $x,y\in R$ and $m\in\mathbb{Z}$:
\begin{enumerate}
    \item $f(0_R)=0_S$
    \item $f(-x)=-f(x)$
    \item $f(x-y)=f(x)-f(y)$
    \item $f(mx)=mf(x)$
\end{enumerate}
since $(R,+)$ is a group.

\subsubsection*{D3.4.7: Ideals}
Let $I\subset R$ where $R$ is a ring. Then
$I$ is an \textbf{ideal} of ring $R$ if:
\begin{enumerate}
    \item $I\neq\emptyset$ and $0_R\in I$
    \item $I$ is closed under subtraction.
    \item $\forall i\in I;\forall r\in R;
    ri,ir\in I$
\end{enumerate}
and we denote $I\trianglelefteq R$.

\subsubsection*{D3.4.11: Ideal of $R$ generated by $T$}
Let $R$ be a commutative ring and $T\subset R$.
Then the ideal of $R$ generated by $T$ is:
$${}_R\langle T\rangle=\left\{\sum_i r_i t_i
:t_i\in T;\forall r_i\in R\right\}$$
where $i\in\{1,\dots,m\}$ and $m\leq|T|$.

\subsubsection*{P3.4.14}
${}_R\langle T\rangle$ is the smallest ideal
containing $T$.

\subsubsection*{D3.4.15: Principle ideal}
An ideal of a commutative ring $R$ is the
\textbf{principle ideal} if:
$$I={}_R\langle t\rangle\hspace{0.05in}\text{where}
\hspace{0.05in}t\in R.$$

\subsubsection*{P3.4.18}
Let $f:R\rightarrow S$ be a ring homomorphism.
Then $\ker(f)$ is an \underline{ideal} of ring $R$ where:
$$\ker(f)=\{r\in R:f(r)=0_S\}$$
and is a subgroup of $(R,+)$.

\subsubsection*{L3.4.21 and L3.4.22}
The set intersection and addition of ideals also form ideals.

\subsubsection*{D3.4.23: Subrings}
A subset $R'\subseteq R$ is a subring of ring $R$ if
$R'$ also satisfies D3.1.1.

\subsubsection*{P3.4.26: Subring test}
$R'\subseteq R$ is a subring of $R$ \textbf{if{}f}
$\forall a,b\in R'$:
\begin{enumerate}
    \item $R'$ has multiplicative identity.
    \item $a-b\in R'$
    \item $ab,ba\in R'$
\end{enumerate}
i.e. that $R'$ is closed under subtraction
and multiplication.

\subsubsection*{P3.4.28}
Let $f:R\rightarrow S$ be a ring homomorphism.
\begin{enumerate}
    \item If $R'$ is a subring of $R$ then
    $f(R')$ and $\text{im}(f)$ are subrings of $S$.

    \item Let $f(1_R)=1_S$. Then:
    $$x\in R^{\times}\implies f(x)\in S^{\times}.$$
\end{enumerate}

\subsubsection*{D3.5.1: Relations}
A \textbf{relation} $R$ on set $X$ is a subset of $X\times X$.
We denote $(x,y)=xRy\in X\times X$.

$R$ is an \textbf{equivalence relation} on set $X$
if $\forall x,y,z\in X$ the following is true:
\begin{enumerate}
    \item Reflexive: $xRx$
    \item Symmetric: $xRy\iff yRx$
    \item Transitive: $(xRy\land yRz)\implies xRz$.
\end{enumerate}

\subsubsection*{D3.5.3: Equivalence classes}
Let $\sim$ be an equivalence relation on $X$.
Then the \textbf{equivalence class} of $x\in X$ is:
$$E(x)=\{z\in X:z\sim x\}\subseteq X$$
where an element of an equivalence class is 
a \textbf{representative} of the class.

\subsubsection*{D3.5.5}
Given an equivalence relation $\sim$ on set $X$,
the \textbf{set of equivalence classes} is:
$$(X\backslash\sim)\deq\{E(x):x\in X\}
\subseteq\mathcal{P}(X).$$
We also define a \underline{surjective} map:
$$\text{can}:X\rightarrow(X/\sim);x\mapsto E(x)$$
known as the \textbf{canonical mapping}.

\subsubsection*{Remark}
A mapping $f:X\rightarrow Z$ is \textbf{well-defined} if
there is an equivalence relation $\sim$ on $X$ such that
$x\sim y\implies f(x)=f(y)$. 

Then there exists a unique mapping $\overline{f}$:
$$\overline{f}:(X/\sim)\rightarrow Z;
E(x)\mapsto f(x)$$
where $f=\overline{f}\circ\text{can}$.

\subsubsection*{D3.6.1: Cosets}
Let $I$ be an ideal of ring $R$. Then:
$$x+I=\{x+i:i\in I\}\subseteq R$$
is the coset of $x$ with respect to $I$ in $R$.

\subsubsection*{Remark}
\begin{enumerate}
    \item $x+I$ is both a left and right coset of $x$
    since $(R,+)$ is Abelian.

    \item Ideals of rings are subgroups.
\end{enumerate}

\subsubsection*{D3.6.3: Factor rings}
Let $I$ be an ideal of ring $R$ and define an
equivalence relation on $R$ where:
$$x\sim y\iff x-y\in I.$$
Then the \textbf{factor ring} of $R$ by $I$ is the
\textcolor{red}{set of cosets} of $I$ in $R$
and denoted as $R/I$:
$$R/I=(R/\sim)$$
for each element is an equivalence class:
\begin{align*}
    E(x)
    &=\{z\in R:z-x\in I\} \\
    &=\{x+i\in R:i\in I\} \\
    &=x+I.
\end{align*}

\subsubsection*{T3.6.4}
Let $I$ be an ideal of ring $R$. Then $R/I$
is a ring where $\forall x,y\in R$:
$$(x+I)\hspace{0.02in}\dot{+}\hspace{0.02in}
(y+I)=(x+y)+I$$
$$(x+I)\cdot(y+I)=xy+I$$
where $x+I,y+I\in R/I$.

\subsubsection*{T3.6.7}
Let $I$ be an ideal of ring $R$. Then:
\begin{enumerate}
    \item $\text{can}:R\rightarrow R/I$ is a surjective
    ring homomorphism with kernel $I$.

    \item Let $f:R\rightarrow S$ where $f(I)=\{0_S\}$ and
    that $f$ is a ring homomorphism.
    
    Then there is
    a unique $\overline{f}:R/I\rightarrow S$
    such that $f=\overline{f}\circ\text{can}$ and that
    $\overline{f}$ is also a ring homomorphism.
\end{enumerate}
\begin{center}
    \includegraphics[scale=0.5]{f01.png}
\end{center}

\subsubsection*{T3.6.9: FIT for rings}
Every ring homomorphism $f:R\rightarrow S$ \\
induces a ring isomorphism:
$$\overline{f}:R/\ker(f)\rightarrow\text{im}(f)$$
where $\overline{f}$ is a \textbf{bijection}. This is
the first isomorphism theorem for rings.

\subsubsection*{D3.7.1: Left modules}
A left module $M$ over a ring $R$ is a \textbf{pair} consisting
of an \textcolor{red}{Abelian group} $(M,\dot{+})$
\underline{and} the following mapping:
$$R\times M\rightarrow M;(r,a)\mapsto ra$$
such that $\forall r,s\in R$ and $\forall a,b\in M$:
$$r(a\hspace{0.03in}\dot{+}\hspace{0.03in}b)
=(ra)\hspace{0.03in}\dot{+}\hspace{0.03in}(rb)$$
$$(r+s)a=(ra)\hspace{0.03in}\dot{+}\hspace{0.03in}(sa)$$
$$r(sa)=(rs)a$$
$$1_R a=a$$
also known as an \textbf{$R$-module}.

\subsubsection*{L3.7.8}
Let $R$ be a ring and $M$ be a $R$-module.
Then $\forall r\in R$ and $\forall a\in M$:
\begin{enumerate}
    \item $0_R a=0_M$
    \item $r0_M=0_M$
    \item $(-r)a=r(-a)=-(ra)$.
\end{enumerate}

\subsubsection*{D3.7.11: Module homomorphisms}
Let $R$ be a ring, $M$ and $N$ be $R$-modules. Then
$f:M\rightarrow N$ is an \textcolor{red}{$R$-homomorphism}
if $\forall r\in R$ and $\forall a,b\in M$:
$$f(a+b)=f(a)+f(b)$$
$$f(ra)=rf(a).$$
$f$ is an \textbf{$R$-isomorphism} if it is \underline{bijective}
and we denote that $M\cong N$.

\subsubsection*{D3.7.15: Submodules}
$M'\subseteq M$ is a submodule if it also satisfies D3.7.11
but restricted to itself.

\subsubsection*{P3.7.20: Submodule test}
Let $M$ be a $R$-module. $M'$ is a submodule of 
$M$ \textbf{if{}f} $\forall a,b\in M$ and $\forall r\in R$:
\begin{enumerate}
    \item $0_M\in M'$
    \item $a-b\in M'$
    \item $ra\in M'$.
\end{enumerate}

\subsubsection*{L3.7.21}
Let $f:M\rightarrow N$ be an $R$-homomorphism.
Then $\ker(f)$ and $\text{im}(f)$ are submodules.

\subsubsection*{D3.7.23: Submodule generated by $T$}
Let $T\subseteq M$ for $M$ is a $R$-module. Then:
$${}_R\langle T\rangle=\left\{\sum_i r_i t_i:
t_i\in T;\forall r_i\in R\right\}$$
where $i\in\{1,\dots,m\}$ and $m\leq|T|$.

Module $N$ is \textbf{cyclic}
if $N={_R}\langle t\rangle$.

\subsubsection*{L3.7.29 and L3.7.30}
Intersecting and adding collections of submodules
also form submodules.

\subsubsection*{D3.7.31: Factor modules}
Let $R$ be a ring, $M$ be a $R$-module 
and $N$ a submodule of $M$. Let $a\in M$. Then the
\textbf{coset of $a$ with respect to $N$ in $M$} is:
$$a+N=\{a+b:b\in N\}$$
Every coset is an equivalent class of the following
equivalence relation:
$$\forall a,b\in M;a\sim b\iff 
a-b\in N$$
and we define the \textbf{factor module} of $M$
by the submodule $N$ as:
$$M/N=(M/\sim)$$
with the following operators:
$$(a+N)\hspace{0.02in}\dot{+}\hspace{0.02in}
(b+N)=(a+b)+N$$
$$r(a+N)=ra+N$$
and additive identity $0_{M/N}=0_M+N$.

\subsubsection*{T3.7.33: FIT for modules}
Let $M$ and $N$ be $R$-modules. Then every
$R$-homomorphism $f:M\rightarrow N$ induces
a $R$-isomorphism:
$$\overline{f}:M/\ker(f)\rightarrow\text{im}(f)$$
where $\overline{f}$ is a bijection.

\subsubsection*{D4.1.1: Symmetric group $S_n$}
The set of bijections from $\{1,\dots,n\}$ to itself
is a group under composition with $n!$ elements, denoted
by $S_n$.

Each bijection $\sigma\in S_n$ is a \textbf{permutation}.

\textbf{Transpositions} are permutations that swap
\textcolor{red}{only two elements} in $\{1,\dots, n\}$.

\subsubsection*{D4.1.2: Inversions}
Given a permutation $\sigma\in S_n$, an inversion is
a pair $(i,j)$ that is interchanged:
$$i<j\hspace{0.05in}\text{and}
\hspace{0.05in}\sigma(j)<\sigma(i)$$
where $i,j\in\{1,\dots, n\}$. The \textbf{length} of
$\sigma$ is the number of inversions:
$$\ell(\sigma)\deq|\{(i,j):i<j\hspace{0.05in}\text{and}
\hspace{0.05in}\sigma(j)<\sigma(i)\}|$$
or the number of crossings in a diagram.
We define the \textbf{sign} of $\sigma$:
$$\text{sgn}(\sigma)=(-1)^{\ell(\sigma)}
=\left\{\begin{array}{ll} +1 & \text{even $\sigma$} \\
-1 & \text{odd $\sigma$.}\end{array}\right.$$

\subsubsection*{Remark}
A cycle $(a_1\dots a_r)$ sends each element to its
\underline{right} \textcolor{red}{except}
$a_r$ which is sent to $a_1$.
$$(a_1 a_2\dots a_r)=(a_1 a_r)(a_1 a_{r-1})\dots(a_1 a_2)$$
Note that order may not be interchanged.

\subsubsection*{D: Group homomorphisms}
Let $G$ and $H$ be groups. Then $f:G\rightarrow H$
is a group homomorphism if $\forall a,b\in G$:
$$f(ab)=f(a)f(b).$$

\subsubsection*{L4.1.5}
$\text{sgn}:S_n\rightarrow\{+1,-1\}$, which is the sign of
permutations, is a group homomorphism:
$$\text{sgn}(\sigma\tau)=\text{sgn}(\sigma)\text{sgn}(\tau)$$
for all $\sigma,\tau\in S_n$.

\subsubsection*{D4.1.6: Alternating groups}
The alternating group is a \underline{subgroup} of $S_n$
and is the kernel of $\text{sgn}$ function:
\begin{align*}
    A_n &\deq \ker(\text{sgn}) \\
    &=\{\sigma\in S_n:\text{sgn}=e_{\{\pm1\}}=+1\}.
\end{align*}

\subsubsection*{D4.2.1: Determinants}
Let $R$ be a \underline{commutative} ring and $n\in\mathbb{N}$.
Then the determinant map is defined as:
$$\text{det}:\text{mat}(n\times n; R)\rightarrow R;$$
$$A\mapsto\sum_{\sigma\in S_n}\text{sgn}(\sigma)
a_{1\sigma(1)}\dots a_{n\sigma(n)}$$
where $(A)_{ij}=a_{ij}$.

\subsubsection*{D4.3.1: Bilinear forms}
Let $U,V$ and $W$ be $F$-vector spaces. Then
a \textbf{bilinear form} on $U\times V$ with values in $W$
is the \textcolor{red}{mapping $H:U\times V\rightarrow W$} where
$\forall\vc{u}_1,\vc{u}_2\in U$, $\forall\vc{v}_1,\vc{v}_2\in V$
and $\forall\lambda\in F$:
$$H(\vc{u}_1+\vc{u}_2,\vc{v}_1)=H(\vc{u}_1,\vc{v}_1)
+H(\vc{u}_2,\vc{v}_1)$$
$$H(\lambda\vc{u}_1,\vc{v}_1)=\lambda H(\vc{u}_1,\vc{v}_1)$$
$$H(\vc{u}_1,\vc{v}_1+\vc{v}_2)=H(\vc{u}_1,\vc{v}_1)
+H(\vc{u}_2,\vc{v}_2)$$
$$H(\vc{u}_1,\lambda\vc{u}_1)=\lambda H(\vc{v}_1,\vc{u}_1)$$
or that every entry of $H$ is \underline{linear}.

Consider a bilinear form $H$ where:
$$H:U\times U\rightarrow W.$$
$H$ is \textbf{symmetric} if $H(\vc{u},\vc{v})=H(\vc{v},\vc{u})$.

$H$ is \textbf{alternating} if $H(\vc{u},\vc{u})=0$, and \\
these must hold for all $\vc{u},\vc{v}\in U$.

\subsubsection*{Remark}
A bilinear form $H$ that satisfies:
$$H(\vc{u},\vc{u})+H(\vc{u},\vc{u})=0$$
is alternating provided $1_F+1_F\neq0_F$.

\subsubsection*{D4.3.3: Multilinear forms}
Let $V_1,\dots,V_n,W$ be $F$-vector spaces. Then the map
$H:V_1\times\dots\times V_n\rightarrow W$ is a
\textbf{multilinear form} if every $V_j\rightarrow W$
is \textcolor{red}{linear} for all $\vc{m}_j\in V_j$.

$H$ is \textbf{alternating} if $\exists i\neq j:\vc{v}_i=\vc{v}_j$
and:
$$H(\vc{v}_1,\dots,\vc{v}_i,\dots,\vc{v}_j,\dots,\vc{v}_n)=0.$$

\subsubsection*{T4.3.6}
Let $F$ be a field. Then the mapping:
$$\text{det}:\text{mat}(n\times n; F)\rightarrow F$$
is the unique alternating multilinear form on $n$-tuples
of \underline{ordered} column vectors of the matrix
under mapping. Furthermore:
$$\text{det}(I_n)=1_F.$$

\subsubsection*{T4.4.1}
Let $A,B\in\text{mat}(n\times n;R)$ where $R$ is a
commutative ring. Then:
$$\det(AB)=\det(A)\det(B).$$

\subsubsection*{T4.4.2}
A square matrix $A$ with entries in a field $F$ \textit{is invertible}
\textbf{if{}f} $\det(A)\neq0$.

\subsubsection*{L4.4.4}
Let $R$ be a commutative ring. Then: \\
$\det(A^T)=\det(A)$ for $A\in\text{mat}(n\times n; R)$.

\subsubsection*{D4.4.6: Cofactors}
Let $R$ be a commutative ring and that $A\in\text{mat}(n\times n; R)$.
Let $i,j\in\{1,\dots,n\}$. We define a \textbf{cofactor} of $A$ as:
$$C_{ij}=(-1)^{i+j}\det(M_{ij})$$
where $M_{ij}$ is a \textit{minor} of $A$, obtained from removing the 
$i^{\text{th}}$ row and $j^{\text{th}}$ column of $A$.

\subsubsection*{T4.4.7: Laplace's expansion}
Let $R$ be a commutative ring, $A=(a_{ij})$ where
$A\in\text{mat}(n\times n; R)$. Then:
\begin{align*}
    \det(A)
    &=\sum_{j=1}^{n}a_{ij}C_{ij} 
    &&\text{($i^{\text{th}}$ row)} \\
    &=\sum_{i=1}^{n}a_{ij}C_{ij}.
    &&\text{($j^{\text{th}}$ column)}
\end{align*}

\subsubsection*{D4.4.8: Adjugate matrices}
Let $R$ be a commutative ring and let $A\in\text{mat}(n\times n;R)$.
Then the \textbf{adjugate matrix} $\text{adj}(A)$ is the $n\times n$
matrix where:
$$\text{adj}(A)_{ij}=C_{ji}.$$

\subsubsection*{T4.4.9: Cramer's rule}
Let $R$ be a commutative ring and let $A\in\text{mat}(n\times n;R)$.
Then:
$$A\cdot\text{adj}(A)=(\det A)I_n.$$

\subsubsection*{C4.4.11}
Let $R$ be a commutative ring and let $A\in\text{mat}(n\times n;R)$.
Then $A$ is \textit{invertible} \textbf{if{}f} $\det(A)\in R^{\times}$.

\subsubsection*{T4.4.14: Jacobi's formula}
Let $A=(a_{ij})$ where $a_{ij}=a_{ij}(t)$. Then:
$$\dv{t}\det(A)=\tr(\text{adj}\hspace{0.01in}A)\dv{A}{t}.$$

\subsubsection*{D4.5.1: Eigenspaces}
Let $f:V\rightarrow V$ be an endomorphism of $F$-vector space $V$.
An \textbf{eigenvalue} $\lambda\in F$ of $f$ is a scalar such that
$f(\vc{v})=\lambda\vc{v}$ where $\vc{v}\neq\vc{0}$. The \textbf{eigenspace}
of map $f$ with eigenvalue $\lambda$ is defined as:
$$E(\lambda,f)=\{\vc{v}\in V:f(\vc{v})=\lambda\vc{v}\}.$$

\subsubsection*{T4.5.4}
Let $V$ be a nonzero \textit{finite} $F$-vector space 
where $F$ is an algebraically closed field. Then each
endomorphism of the vector space $V$ has an eigenvalue.

\subsubsection*{D4.5.6: Characteristic polynomials}
Let $R$ be a commutative ring and let $A\in\text{mat}(n\times n;R)$.
The \textbf{characteristic polynomial} of matrix $A$ is defined as:
$$\chi_A(x)\deq\det(xI_n-A)\in R[x].$$

\subsubsection*{T4.5.8}
Let $F$ be a field and $A\in\text{mat}(n\times n;F)$. Then the
eigenvalues of the linear map $A:F^n\rightarrow F^n$ are the
roots of $\chi_A$.

\subsubsection*{P4.6.1: Triangularisability}
Let $f:V\rightarrow V$ be an endomorphism of a \textit{finite} 
$F$-vector space. Then the following two statements are equivalent:
\begin{enumerate}
    \item Vector space $V$ has an \textit{ordered} basis
    $B=\{\vc{v}_1,\dots,\vc{v}_n\}$ such that:
    \begin{align*}
        f(\vc{v}_1) &=a_{11}\vc{v}_1 \\
        &\dots \\
        f(\vc{v}_n) &=a_{1n}\vc{v}_1+\dots+a_{nn}\vc{v}_n
    \end{align*}
    or that its representation matrix is:
    $${}_B[f]_B=\begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \\
    0 & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & a_{nn}\end{pmatrix}.$$
    i.e. $f$ is \textbf{triangularisable}.

    \item $\chi_f(x)$ may be written as a \textit{product of linear factors}
    in $F[x]$.
\end{enumerate}

\subsubsection*{D4.6.6}

\subsubsection*{L4.6.9}

\subsubsection*{T4.6.10: Cayley-Hamilton}

\subsubsection*{D4.7.5: Markov matrices}

\subsubsection*{L4.7.6}

\subsubsection*{T4.7.10}

\newcolumn

\subsubsection*{D5.1.1: Inner product spaces}
real and complex

\subsubsection*{D5.1.5: Inner product norm}

\subsubsection*{D5.1.7: Orthonormal family}
include explicit calculation

\subsubsection*{T5.1.10}

\subsubsection*{D5.2.1}

\subsubsection*{P5.2.2}

\subsubsection*{D5.2.3: Orthogonal projections}

\subsubsection*{P5.2.4}

\subsubsection*{T5.2.5: Cauchy-Schwarz}

\subsubsection*{C5.2.6}

\subsubsection*{T5.2.7}

\subsubsection*{Remark: Gram-Schmidt}

\end{multicols*}

\end{document}