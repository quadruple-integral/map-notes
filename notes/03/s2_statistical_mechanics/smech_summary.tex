\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{parskip}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{physics}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{mathtools}

% commands
\newcommand{\deq}{\vcentcolon=}
\newcommand{\idd}{\text{Ä‘}}
\newcommand{\nimplies}{\centernot\implies}
\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\pr}{\mathbb{P}}

% margin settings
\geometry{
    a4paper,
    left=7mm,
    right=7mm,
    top=2cm,
    bottom=7mm
}

% testing
\usepackage{blindtext}

% proof environments
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark} % unnumbered remarks

% header and footer
\pagestyle{fancy}
\fancyfoot{} % removes footer
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\fancyhead[L]{Statistical mechanics}
\fancyhead[R]{\thepage}

\begin{document}

\begin{multicols*}{3}
% starred environment ensures text remains in same column
\noindent

\subsubsection*{Probability distributions}
The probablity of an event in a trial is:
$$\pr(\text{event})\deq\lim_{N\rightarrow\infty}\frac{n}{N}$$
given $n$ occurrences in $N$ trials. \\
For discrete probabilities:
$$\sum_{i=1}^{q}\pr(i)=1$$
$$\pr(\text{$i$ or $j$})=\pr(i)+\pr(j)$$
$$\pr(\text{$i$ and $j$})=\pr(i)\pr(j).$$
Given continuous random variables:
$$\pr([x,x+\dd x])=P(x)\dd x$$
for $P$ is the probability density function:
$$\int_{-\infty}^{\infty}P(x)\dd x=1.$$
We define the \textbf{mean} and \textbf{variance} as:
$$\overline{x}=\sum_{i=1}^{q}x_i P_i
\hspace{0.05in}\text{or}\hspace{0.05in}
\int_{-\infty}^{\infty}xP(x)\dd x$$
\begin{align*}
    \overline{\Delta x^2}
    &=\sum_{i=1}^{q}(x_i-\overline{x})^2 P_i \\
    &=\int_{-\infty}^{\infty}
    (x-\overline{x})^2P(x)\dd x \\
    &=\overline{x^2}-(\overline{x})^2.
\end{align*}
The \textbf{standard deviation} is the square root
of the variance $\bigl(\overline{\Delta x^2}\bigr)^{1/2}$ and:
$$\overline{f(x)}=
\int_{-\infty}^{\infty}f(x)P(x)\dd x.$$

\subsubsection*{Binomial distribution}
The probability of observing $n$ events 
each with probability $p$ in $N$ trials is:
$$P_n=\begin{pmatrix}
N \\ n\end{pmatrix}p^n(1-p)^{N-n}$$
where
$\displaystyle\begin{pmatrix}N \\ n\end{pmatrix}
=\frac{N!}{n!(N-n)!}$ with:
$$\overline{n}=Np
\hspace{0.07in}\text{and}\hspace{0.07in}
\overline{\Delta n^2}=Np(1-p)$$
since we have that:
$$(a+b)^N=\sum_{n=0}^{N}
\begin{pmatrix}N \\ n\end{pmatrix}a^n b^{N-n}$$
\begin{align*}
    f(\alpha)
    &=\sum_{n=0}^{N}\begin{pmatrix}N \\ n\end{pmatrix}
    (p\alpha)^n(1-p)^{N-n} \\
    &=(p\alpha+1-p)^N.
\end{align*}
Note that $\begin{pmatrix}N \\ n\end{pmatrix}$
denotes ways to pick $n$ items 
from $N$ items.
For large $N$:
$$\ln(N!)\approx N\ln(N)-N$$
known as \textbf{Stirling's approximation}.

\newcolumn

We also define the \textbf{fractional deviation}
as the deviation on the scale of the mean:
$$\frac{\bigl(\overline{\Delta x^2}\bigr)^{1/2}}{\overline{x}}
=\frac{1}{N^{1/2}}.$$

\subsubsection*{Taylor expansions}
Let $s(n)$ be expanded at $n=a$:
\begin{align*}
    s(n)&=s(a)+s'(a)(n-a) \\
    &\quad+\frac{1}{2}s''(a)(n-a)^2+\mathcal{O}[(n-a)^3].
\end{align*}

\subsubsection*{Poisson distribution}
Let $N\gg n$ and let $p$ be the probability of 
an event in a trial. Assume that as $N\rightarrow\infty$,
$p\rightarrow0$. Under such conditions
the binomial probability of observing $n$ events 
in $N$ trials is:
\begin{align*}
    P_n
    &\approx(\overline{n})^n\frac{\exp(-\overline{n})}{n!}
\end{align*}
with mean and variance $Np$.

\subsubsection*{Gaussian distribution}
Let $N$ be very large. Then the binomial distribution
becomes Gaussian:
$$P_n\approx\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(n-Np)^2}{2\sigma^2}\right)$$
via Stirling's approximation and Taylor expansions
with variance $\sigma^2=Np(1-p)$ and mean $\mu=Np$.

\subsubsection*{Microstates and macrostates}
A \textbf{microstate} is a complete specification of
\textcolor{red}{all degrees of freedoms} in a system,
with respect to a microscopic model.

A \textbf{macrostate} is a limited description
by the values of observables, like pressure.

We assume that the molecules are
weakly interacting. (no interaction potentials)

\subsubsection*{Boltzmann law}
Consider a \textbf{microcanonical ensemble} with
\textcolor{red}{fixed} $N$ and $E$.
The Boltzmann law defines the entropy
for isolated systems:
$$S(N,E,\{\alpha\})\deq k_B\ln\Bigl[
\Omega(N,E,\{\alpha\})\Bigr]$$
$$k_B=1.381\times10^{-23}\text{J}\text{K}^{-1}$$
where $\Omega$ is the corresponding number of microstates
to a macrostate defined by a set of observables $\{\alpha\}$.
The probability an isolated system with
\underline{macrostate} is:
$$\mathbb{P}(\alpha^*_i)=
\frac{\Omega(\alpha^*_i)}{\Omega(\{\alpha\})}.$$
Maximum entropy is at the equilibrium state 
since it has the largest weight $\Omega$. Hence an
isolated system is \underline{most likely}
to be found at equilibrium.

\subsubsection*{Two-state model magnets}
Consider an array of $N$ magnetic dipoles 
and total energy $E$
that is subject to a magnetic field $\vc{H}$.
$$\{\uparrow\downarrow\uparrow\uparrow\dots
\downarrow\downarrow\uparrow\uparrow\}$$
Define $n$ to be the number of
dipoles with energy $\epsilon_{\uparrow}=+mH$
(excited state) and the remaining
in $\epsilon_{\downarrow}=-mH$ (ground state).

Since we can write the total energy $E$ as:
$$mH\bigl(n-(N-n)\bigr)=E$$
$$\therefore n=\frac{1}{2}\left(N+\frac{E}{mH}\right)$$
and the \textbf{weight} of this macrostate is:
$$\Omega(N,E,n)=\begin{pmatrix}
N \\ n\end{pmatrix}.$$
If $N\gg1$ we use Stirling's approximation
and define $x=n/N$: 
$$\Omega(N,E,n)\approx\exp\Bigl[N s(x)\Bigr]$$
$$s(x)=-(1-x)\ln(1-x)-x\ln x.$$
\begin{center}
    \includegraphics*[scale=0.5]{f00.png}
\end{center}
For in the $s(x)$ plot above
our end points are computed via limits.

Now let the number of excited dipoles be
$n=N/2$ and denote $n_L$ as the number excited
dipoles in the left.
$$\{\underbrace{\dots\uparrow
\downarrow\uparrow\dots}_{\text{$n_L$}}|
\dots\downarrow\downarrow\uparrow\dots\}$$
The weight of macrostate $n_L$ now is:
$$\Omega(N,\textcolor{red}{E=0},n_L)=
\begin{pmatrix}N/2 \\ n_L\end{pmatrix}
\begin{pmatrix}N/2 \\ n-n_L\end{pmatrix}$$
which under large $N$ becomes:
$$\frac{1}{N}\ln\Bigl[\Omega(N,0,n_L)\Bigr]\approx s(y)$$
for $y=n_L/(N/2)$. If $N\rightarrow\infty$:
$$\Omega(N,0,n_L)=\left\{\begin{array}{ll}
0 &y\neq0.5 \\ 2^N &y=0.5\end{array}\right.$$
or that $n_L=N/4$ exactly for large $N$.

\subsubsection*{Entropy}
Entropy is a \textbf{measure of disorder} in a system.
For subsystems in \underline{equilibrium}:
\begin{align*}
    &\Omega(N,E)=\Omega(N_1,E_1)\Omega(N_2,E_2) \\
    &\implies S=S_1+S_2.
\end{align*}
If $E_1\rightarrow E_1+\dd E_1$ and
$E_2\rightarrow E_2-\dd E_1$:
$$\dd S=\left(\frac{\partial S_1}{\partial E_1}
-\frac{\partial S_2}{\partial E_2}\right)\dd E_1=0$$
since overall we have an isolated system.
i.e. objects in thermal equilibrium have the same temperature:
\begin{align*}
    &\dd E=T\dd S-P\dd V \\
    &\implies\frac{\partial S_i}{\partial E_i}
    \deq\frac{1}{T_i}
\end{align*}
since fixed number of particles $N$ 
in an isolated system implies a fixed volume $V$.

i.e. temperature is the \textcolor{red}{ratio of change}
of $S$ and $E$ of a system!
If there exists a temperature gradient:
$$\dd S=\left(\frac{1}{T_1}-\frac{1}{T_2}\right)
\dd E_1>0$$ where $T_1>T_2$ implies negative $\dd E_1$.

\subsubsection*{Boltzmann distribution}
Consider a \textbf{canonical ensemble} with fixed particles $N$ but
\textcolor{red}{changing energy $E$} in thermal equilibrium
at temperature $T$.

Then the \underline{probability} of \textbf{an} energy state $E_i$ for
this canonical ensemble is:
$$\mathbb{P}(E_i)=\frac{1}{Z}\exp(-\beta E_i)$$
$$Z=\sum_j\exp(-\beta E_j)
\hspace{0.05in}\text{and}\hspace{0.05in}
\beta=\frac{1}{k_B T}.$$
\textbf{Partition function} $Z$
is the sum of \textcolor{red}{all} microstates $E_i$ of the ensemble.

\subsubsection*{Free energy minimisation}
The \textbf{mean energy} is computed as:
\begin{align*}
    \overline{E}
    &=\sum_i E_i\hspace{0.01in}\mathbb{P}(E_i) \\
    &=-\frac{1}{Z}\sum_i\left(\frac{\partial}{\partial\beta}
    \exp(-\beta E_i)\right) \\
    &=-\frac{1}{Z}\frac{\partial Z}{\partial\beta}
    =-\frac{\partial\ln Z}{\partial\beta} \\
    &=k_B T^2\frac{\partial\ln Z}{\partial T}
\end{align*}
and \textbf{heat capacity} is defined as:
\begin{align*}
    C
    &\deq\frac{\partial\overline{E}}{\partial T}
    =-\frac{1}{k_B T^2}
    \frac{\partial\overline{E}}{\partial\beta} \\
    &=\frac{\overline{(\Delta E)^2}}{k_B T^2}
\end{align*}
since $\overline{(\Delta E)^2}=\overline{E^2}
-\overline{E}^2$.

\newcolumn

For every macrostate $E$ 
there corresponds $\Omega(E)$ microstates:
$$\overline{E}=\sum_{E}\Bigl(\Omega(E)\cdot E\Bigr)
\Bigl[\frac{1}{Z}\exp(-\beta E)\Bigr]$$
and the probability of macrostate $E$ is:
\begin{align*}
    \mathbb{P}(E)
    &=\frac{1}{Z}\Omega(E)\exp(-\beta E) \\
    &=\frac{1}{Z}\exp(-\beta F)
\end{align*}
$$Z=\sum_E\Omega(E)\exp(-\beta E)$$
where $F=E-TS$. \textbf{Free energy} $F$ is
\textcolor{red}{minimised} by the
\underline{equilibrium} state $\overline{E}$.

If $N_1$ is very large, $\mathbb{P}(\overline{E})\rightarrow1$ and:
\begin{align*}
    Z
    &\approx\Omega(\overline{E})\exp(-\beta\overline{E})
    \cdot\mathcal{O}[N^{1/2}] \\
    &=\exp(-\beta F)\cdot\mathcal{O}[N^{1/2}]
\end{align*}
for here $F=\overline{E}-T S(T)$. Importantly:
$$F(T)=-k_B T\ln Z$$
$$\overline{E}(T)=k_B T^2\frac{\partial\ln Z}{\partial T}$$
$$S(T)=k_B\ln Z+\frac{\overline{E}(T)}{T}.$$

\subsubsection*{Weakly interacting constituents}
Consider a system of $N$ particles.
In the absence of interaction potentials given a microstate
$r$ with total energy $E_r$:
$$E_r=\epsilon_{i_1}+\dots+\epsilon_{i_N}$$
for $\epsilon_{i_j}$ is the $j^{th}$ particle in the $i^{th}$ state
and has the following partition function:
$$Z=[Z(1)]^N$$
$$Z(1)=\sum_i\exp(-\beta\epsilon_i).$$
The \textbf{probability} of particle $1$ to exist at 
state $j$ is given by:
\begin{align*}
    \mathbb{P}(\epsilon_{j_1})
    &=\sum_{i_2,\dots,i_N}\frac{\exp\Bigl[-\beta
    (\epsilon_{j_1}+\epsilon_{i_2}+\dots)\Bigr]}{Z} \\
    &=\frac{\exp(-\beta\epsilon_{j_1})}{Z(1)}
\end{align*}
assuming particles can be distinguished.

\subsubsection*{Classical solids}
A classical $3d$ solid with $N$ particles has
spring oscillators which connects every particle.
Every oscillator has energy:
$$\epsilon=\frac{1}{2}k\vc{x}^2+\frac{1}{2}m\vc{v}^2$$
with $6$ degrees of freedom. Then:
$$\overline{E}=3Nk_B T$$
and is known as the Dulong--Petit law.

\subsubsection*{Einstein's model of solids}
Consider a system of $N$ particles which are weakly interacting.
If every particle is modelled after the \textbf{same}
$3d$ quantum oscillator with frequency $\omega$ then:
\begin{align*}
    Z &= [Z_{1d}(1)]^{3N} \\
    Z_{1d}(1) 
    &=\sum_{n=0}^{\infty}\exp(-\beta\epsilon_n) \\
    &=\frac{\exp(-\frac{x}{2})}{1-\exp(-x)}
\end{align*}
where $\epsilon_n$ is the one dimensional harmonic oscillator
at the $n^{th}$ energy state:
$$\epsilon_n=\left(n+\frac{1}{2}\right)\hbar\omega$$
and $x=\beta\hbar\omega$. We have also used:
$$\sum_{n=0}^{\infty}a^n=\frac{1}{1-a}
\hspace*{0.05in}\text{where}\hspace*{0.05in}|a|<1.$$
Then the system the following properties:
\begin{align*}
    \overline{E} &=3N\overline{\epsilon}
    =3N\cdot-\frac{\partial}{\partial\beta}\ln\Bigl[
    Z_{1d}(1)\Bigr] \\
    &=3N\hbar\omega\left[\frac{\exp(-x)}
    {1-\exp(-x)}+\frac{1}{2}\right] \\
    C_V &=\left(\frac{\partial\overline{E}}{\partial T}\right)_V
    =\left(\frac{\partial x}{\partial T}\right)_{\omega}
    \left(\frac{\partial\overline{E}}{\partial x}\right)_{\omega} \\
    &=3Nk_B\frac{x^2\exp(x)}{\bigl(\exp(x)-1\bigr)^2} \\
    S_{vib} &=3Nk_B\left[\frac{x}{e^x-1}-\ln(1-e^{-x})\right]
\end{align*}
with \textbf{characteristic temperature} $T^*$:
$$T^*=\frac{\hbar\omega}{k_B}$$
and is weakly interacting when $T\gg T^*$.

\subsubsection*{Ideal gases}

\end{multicols*}

\end{document}